/home/star/anaconda3/envs/lot_prompt/bin/python /home/star/文档/wy/lot_prompt/auto_run.py
INFO:gensim.models.keyedvectors:loading projection weights from /home/star/文档/wy/lot_prompt/data/crawl-300d-2M-subword.vec
INFO:gensim.utils:KeyedVectors lifecycle event {'msg': 'loaded (2000000, 300) matrix of type float32 from /home/star/文档/wy/lot_prompt/data/crawl-300d-2M-subword.vec', 'binary': False, 'encoding': 'utf8', 'datetime': '2023-11-19T09:41:52.658887', 'gensim': '4.3.2', 'python': '3.9.18 (main, Sep 11 2023, 13:41:44) \n[GCC 11.2.0]', 'platform': 'Linux-6.2.0-36-generic-x86_64-with-glibc2.35', 'event': 'load_word2vec_format'}
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_0.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --this_run_unicode 6176531173
load_model_time:  117.82039999961853
0
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_0.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4829.92it/s]
tokenizing: 80it [00:00, 4696.21it/s]
tokenizing: 6076it [00:01, 5445.10it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 0, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Epoch 2, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 3, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Epoch 4, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 5, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 6, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 7, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 8, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 9, val_acc 0.8
train_time:  319.832731962204
Test: 100%|██████████| 95/95 [09:51<00:00,  6.22s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_1.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.826366030283081	Pre: 0.8297298126833593	Rec: 0.826366030283081	F1s: 0.8240442350024467




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_0.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.66459321975708
agnewstitle_alltime:  939.3060677051544
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --this_run_unicode 6176531173
1
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4692.27it/s]
tokenizing: 80it [00:00, 4919.86it/s]
tokenizing: 6076it [00:01, 5542.69it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Epoch 0, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 1, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 3, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 4, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 5, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 6, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
Epoch 7, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 8, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 9, val_acc 0.8125
train_time:  317.5107822418213
Test: 100%|██████████| 95/95 [09:47<00:00,  6.18s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_2.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.825378538512179	Pre: 0.8256362870590748	Rec: 0.825378538512179	F1s: 0.8254825168387868




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_1.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.60456418991089
agnewstitle_alltime:  933.3520951271057
2
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
tokenizing: 0it [00:00, ?it/s]##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4701.94it/s]
tokenizing: 80it [00:00, 4981.58it/s]
tokenizing: 6076it [00:01, 5469.54it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Epoch 0, val_acc 0.6875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.7125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 2, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 3, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 4, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 5, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 6, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 7, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 8, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 9, val_acc 0.775
train_time:  326.8893926143646
Test: 100%|██████████| 95/95 [09:49<00:00,  6.21s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_3.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8224160631994734	Pre: 0.8235171272093949	Rec: 0.8224160631994734	F1s: 0.8217970667066381




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_2.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.678792238235474
agnewstitle_alltime:  944.9615395069122
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --this_run_unicode 6176531173
3
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4764.22it/s]
tokenizing: 80it [00:00, 5061.61it/s]
tokenizing: 6076it [00:01, 5564.43it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 0, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.9375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Epoch 2, val_acc 0.925
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 3, val_acc 0.8875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 4, val_acc 0.9125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 5, val_acc 0.9125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 6, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 7, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 8, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 9, val_acc 0.9
train_time:  318.69408559799194
Test: 100%|██████████| 95/95 [09:51<00:00,  6.22s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_4.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8095786701777485	Pre: 0.8170364223248295	Rec: 0.8095786701777485	F1s: 0.8114586882749043




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_3.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.499345779418945
agnewstitle_alltime:  938.126683473587
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --this_run_unicode 6176531173
4
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4650.46it/s]
tokenizing: 80it [00:00, 5102.25it/s]
tokenizing: 6076it [00:01, 5540.61it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 0, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 2, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
Epoch 3, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 4, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 5, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 6, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 7, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 8, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 9, val_acc 0.8375
train_time:  327.67220759391785
Test: 100%|██████████| 95/95 [09:49<00:00,  6.21s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_5.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8273535220539829	Pre: 0.8292020507262998	Rec: 0.8273535220539829	F1s: 0.8273091994998306




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_4.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.55432939529419
agnewstitle_alltime:  945.7147843837738
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --this_run_unicode 6176531173
5
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [21, 22, 26, 34]
tokenizing: 80it [00:00, 4930.27it/s]
tokenizing: 80it [00:00, 4768.15it/s]
tokenizing: 6076it [00:01, 5558.95it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.91s/it]
Epoch 0, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 1, val_acc 0.8875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 2, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 3, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 4, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 5, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 6, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 7, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 8, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 9, val_acc 0.8625
train_time:  321.39511489868164
Test: 100%|██████████| 95/95 [09:48<00:00,  6.20s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_6.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8293285055957867	Pre: 0.8304517974765182	Rec: 0.8293285055957867	F1s: 0.828787486695945




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_5.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  21.61115860939026
agnewstitle_alltime:  937.630136013031
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --this_run_unicode 6176531173
6
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 23, 26, 34]
tokenizing: 80it [00:00, 4723.12it/s]
tokenizing: 80it [00:00, 4816.89it/s]
tokenizing: 6076it [00:01, 5506.36it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 0, val_acc 0.6625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.7125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 2, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 3, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 4, val_acc 0.7125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 5, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 6, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 7, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 8, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 9, val_acc 0.7375
train_time:  322.8661286830902
Test: 100%|██████████| 95/95 [09:51<00:00,  6.23s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_7.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8367346938775511	Pre: 0.8360625266817141	Rec: 0.8367346938775511	F1s: 0.835663249412316




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_6.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.694803476333618
agnewstitle_alltime:  942.9428701400757
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --this_run_unicode 6176531173
7
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 23, 26, 34]
tokenizing: 80it [00:00, 4697.00it/s]
tokenizing: 80it [00:00, 5072.55it/s]
tokenizing: 6076it [00:01, 5561.58it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 0, val_acc 0.625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 1, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 2, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.81s/it]
Epoch 3, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 4, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 5, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 6, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 7, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 8, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 9, val_acc 0.75
train_time:  317.8477432727814
Test: 100%|██████████| 95/95 [09:47<00:00,  6.18s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_8.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.7779789335088875	Pre: 0.8080992928013119	Rec: 0.7779789335088875	F1s: 0.7788130323318335




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_7.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  22.066137075424194
agnewstitle_alltime:  932.7672843933105
8
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 35]
tokenizing: 80it [00:00, 4722.45it/s]
tokenizing: 80it [00:00, 5003.94it/s]
tokenizing: 6076it [00:01, 5546.63it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 0, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.77s/it]
Epoch 1, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.78s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 3, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.74s/it]
Epoch 4, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 5, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.79s/it]
Epoch 6, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.75s/it]
Epoch 7, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 8, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.76s/it]
Epoch 9, val_acc 0.7375
train_time:  319.84822154045105
Test: 100%|██████████| 95/95 [09:43<00:00,  6.14s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_9.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8219223173140224	Pre: 0.8252335857707319	Rec: 0.8219223173140224	F1s: 0.8229027238027309




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_8.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  18.547967672348022
agnewstitle_alltime:  927.6093933582306
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --this_run_unicode 6176531173
9
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
tokenizing: 80it [00:00, 5114.54it/s]
##Num of label words for each label: [22, 24, 26, 35]
tokenizing: 80it [00:00, 5483.11it/s]
tokenizing: 6076it [00:01, 5935.95it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 0, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 1, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 2, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Epoch 3, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 4, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 5, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 6, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 7, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Epoch 8, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 9, val_acc 0.825
train_time:  301.09307837486267
Test: 100%|██████████| 95/95 [09:15<00:00,  5.85s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_10.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.7946017116524029	Pre: 0.7982733789866079	Rec: 0.7946017116524029	F1s: 0.795203104943564




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_9.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  17.771769046783447
agnewstitle_alltime:  879.7097871303558
10
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 35]
tokenizing: 80it [00:00, 5059.17it/s]
tokenizing: 80it [00:00, 5416.90it/s]
tokenizing: 6076it [00:01, 5741.25it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 0, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 1, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 3, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 4, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 5, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Epoch 6, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 7, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 8, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 9, val_acc 0.825
train_time:  306.5370388031006
Test: 100%|██████████| 95/95 [09:17<00:00,  5.87s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_11.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8275181040157998	Pre: 0.8307681292174794	Rec: 0.8275181040157998	F1s: 0.8284173731998332




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_10.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  18.683185338974
agnewstitle_alltime:  888.5530772209167
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --this_run_unicode 6176531173
11
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 36]
tokenizing: 80it [00:00, 5126.96it/s]
tokenizing: 80it [00:00, 5358.85it/s]
tokenizing: 6076it [00:01, 5845.02it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Epoch 0, val_acc 0.7
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 1, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 3, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Epoch 4, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Epoch 5, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.56s/it]
Epoch 6, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 7, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 8, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Epoch 9, val_acc 0.775
train_time:  305.55325627326965
Test: 100%|██████████| 95/95 [09:17<00:00,  5.87s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_12.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.82587228439763	Pre: 0.8312941984790418	Rec: 0.82587228439763	F1s: 0.8262595536330134




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_11.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  18.54668354988098
agnewstitle_alltime:  887.0904541015625
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --this_run_unicode 6176531173
12
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
tokenizing: 80it [00:00, 5157.78it/s]
##Num of label words for each label: [22, 24, 26, 36]
tokenizing: 80it [00:00, 5460.53it/s]
tokenizing: 6076it [00:01, 5827.96it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Epoch 0, val_acc 0.6625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Epoch 1, val_acc 0.7125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Epoch 2, val_acc 0.7125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 3, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 4, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 5, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 6, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 7, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Epoch 8, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Epoch 9, val_acc 0.7375
train_time:  307.9561405181885
Test: 100%|██████████| 95/95 [09:21<00:00,  5.91s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_13.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8013495720868993	Pre: 0.8063577038788176	Rec: 0.8013495720868993	F1s: 0.7995027754584215




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_12.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  18.277689933776855
agnewstitle_alltime:  893.3573532104492
13
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 36]
tokenizing: 80it [00:00, 5261.46it/s]
tokenizing: 80it [00:00, 5517.19it/s]
tokenizing: 6076it [00:01, 5800.81it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 0, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 1, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.63s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.60s/it]
Epoch 3, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 4, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.57s/it]
Epoch 5, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.62s/it]
Epoch 6, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.59s/it]
Epoch 7, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]
Epoch 8, val_acc 0.85
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.58s/it]
Epoch 9, val_acc 0.85
train_time:  301.0305140018463
Test: 100%|██████████| 95/95 [09:19<00:00,  5.89s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_14.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.803982883475971	Pre: 0.8060244227288793	Rec: 0.803982883475971	F1s: 0.804681217427703




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_13.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  23.98973321914673
agnewstitle_alltime:  890.5395457744598
14
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 36]
tokenizing: 80it [00:00, 4798.70it/s]
tokenizing: 80it [00:00, 4997.46it/s]
tokenizing: 6076it [00:01, 5451.08it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 0, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 1, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 2, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 3, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
Epoch 4, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 5, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 6, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 7, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 8, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.80s/it]
Epoch 9, val_acc 0.7375
train_time:  325.3166980743408
Test: 100%|██████████| 95/95 [09:58<00:00,  6.30s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_15.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8240618828176431	Pre: 0.8237261356005185	Rec: 0.8240618828176431	F1s: 0.8235801511869073




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_14.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  23.21438694000244
agnewstitle_alltime:  953.467104434967
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --this_run_unicode 6176531173
15
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [22, 24, 26, 36]
tokenizing: 80it [00:00, 4482.59it/s]
tokenizing: 80it [00:00, 4996.64it/s]
tokenizing: 6076it [00:01, 5290.45it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 0, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 1, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 2, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 3, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 4, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 5, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 6, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 7, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 8, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 9, val_acc 0.8
train_time:  327.44821524620056
Test: 100%|██████████| 95/95 [10:00<00:00,  6.32s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_16.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8271889400921659	Pre: 0.8270019095300795	Rec: 0.8271889400921659	F1s: 0.8259980207221249




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_15.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  24.960893869400024
agnewstitle_alltime:  958.7562992572784
16
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [23, 24, 26, 36]
tokenizing: 80it [00:00, 4607.54it/s]
tokenizing: 80it [00:00, 4929.04it/s]
tokenizing: 6076it [00:01, 5316.35it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Epoch 0, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 1, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 2, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 3, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 4, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 5, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 6, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 7, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 8, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 9, val_acc 0.775
train_time:  332.4306972026825
Test: 100%|██████████| 95/95 [10:01<00:00,  6.33s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_17.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8164911125740619	Pre: 0.8147867467288636	Rec: 0.8164911125740619	F1s: 0.8147757746199613




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_16.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  24.933862447738647
agnewstitle_alltime:  964.5506598949432
17
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [24, 25, 26, 36]
tokenizing: 80it [00:00, 4576.50it/s]
tokenizing: 80it [00:00, 4846.67it/s]
tokenizing: 6076it [00:01, 5507.09it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 0, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 1, val_acc 0.7875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 2, val_acc 0.7375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 3, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 4, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.87s/it]
Epoch 5, val_acc 0.75
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 6, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 7, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 8, val_acc 0.725
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 9, val_acc 0.7625
train_time:  324.3531811237335
Test: 100%|██████████| 95/95 [09:56<00:00,  6.28s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_18.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.7903225806451613	Pre: 0.7931920934356694	Rec: 0.7903225806451613	F1s: 0.7891714173087319




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_17.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  23.737931966781616
agnewstitle_alltime:  950.9541661739349
18
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [24, 25, 26, 36]
tokenizing: 80it [00:00, 4453.26it/s]
tokenizing: 80it [00:00, 5002.60it/s]
tokenizing: 6076it [00:01, 5388.89it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Epoch 0, val_acc 0.8375
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
Epoch 1, val_acc 0.875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 2, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 3, val_acc 0.8625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 4, val_acc 0.875
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 5, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.88s/it]
Epoch 6, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 7, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 8, val_acc 0.9
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 9, val_acc 0.9
train_time:  333.1335265636444
Test: 100%|██████████| 95/95 [09:59<00:00,  6.31s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_19.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8415075707702436	Pre: 0.842600293377068	Rec: 0.8415075707702436	F1s: 0.8409586934811464




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_18.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
开始去重
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  23.464041233062744
agnewstitle_alltime:  961.9111227989197
19
python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_20.csv --this_run_unicode 6176531173
INFO:root:Executing command: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_20.csv --this_run_unicode 6176531173
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.
  warnings.warn(
Some weights of the model checkpoint at /home/star/文档/wy/lot_prompt/model/roberta were not used when initializing RobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
- This IS expected if you are initializing RobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing RobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
##Num of label words for each label: [24, 25, 26, 36]
tokenizing: 80it [00:00, 4930.20it/s]
tokenizing: 80it [00:00, 4619.28it/s]
tokenizing: 6080it [00:01, 5339.70it/s]
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 0, val_acc 0.775
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 1, val_acc 0.7625
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.82s/it]
Epoch 2, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 3, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 4, val_acc 0.8125
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.89s/it]
Epoch 5, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.83s/it]
Epoch 6, val_acc 0.825
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.85s/it]
Epoch 7, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.84s/it]
Epoch 8, val_acc 0.8
Valid: 100%|██████████| 2/2 [00:07<00:00,  3.86s/it]
Epoch 9, val_acc 0.8
train_time:  328.5193235874176
Test: 100%|██████████| 95/95 [09:58<00:00,  6.30s/it]
====================
dataset agnewstitle	temp 0	seed 120	shot 20	verb upt	cali False	filt none	maxsplit -1	kptw_lr 0.06	test: ./datasets/TextClassification/agnewstitle/test_20/test_20.csvlearning_rate: 3e-05batch_size: 64
Acc: 0.8171052631578948	Pre: 0.8209205801422612	Rec: 0.8171052631578948	F1s: 0.8161406490561453




INFO:root:Command executed successfully: python fewshot.py --result_file ./output_fewshot.txt --dataset agnewstitle --template_id 0 --seed 120 --batch_size 64 --shot 20 --learning_rate 3e-5 --verbalizer upt --train_datasets ./datasets/TextClassification/agnewstitle/test_20/test_19.csv --test_datasets ./datasets/TextClassification/agnewstitle/test_20/test_20.csv --this_run_unicode 6176531173
Some weights of the model checkpoint at ./model/bert_case were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'bert.pooler.dense.bias', 'bert.pooler.dense.weight']
- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/home/star/anaconda3/envs/lot_prompt/lib/python3.9/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning
  super()._check_params_vs_input(X, default_n_init=10)
开始去重
写入success
写入success
写入success
写入success
agnewstitle_verbalizer_time:  23.690495252609253
agnewstitle_alltime:  956.2727565765381
all_time:  18785.42377305031

Process finished with exit code 0
